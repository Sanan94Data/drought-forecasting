# -*- coding: utf-8 -*-
"""Final_ML_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iA66EPbrUsXmA_VsYSlf4fpqFDqIaAXy
"""

"""
Data Preparation and Drought Labeling Script
--------------------------------------------
This script merges three climate datasets (precipitation, evapotranspiration, and PDSI),
normalizes the PDSI values, generates a binary drought label, and outputs a merged dataset
for use in machine learning models.

Drought Label Rule:
- Drought (label=1) if normalized PDSI < -0.5
- No Drought (label=0) otherwise

Output: prepared_drought_dataset.csv
"""

import pandas as pd

# Load datasets
df_precip = pd.read_csv("CHIRPS_Monthly_Prairies_1982_2022.csv")
df_evap = pd.read_csv("ERA5_Evapotranspiration_Prairies_1982_2022.csv")
df_pdsi = pd.read_csv("PDSI_TerraClimate_Prairies_1982_2022.csv")

# Merge datasets on 'year' and 'month'
df_merged = df_precip.merge(df_evap, on=["year", "month"])
df_merged = df_merged.merge(df_pdsi, on=["year", "month"])

# Normalize PDSI values (MANDATORY step)
df_merged["pdsi"] = df_merged["pdsi"] / 100  # Scales values to ~[-4, 4]

# Generate binary drought label
df_merged["drought_label"] = df_merged["pdsi"].apply(lambda x: 1 if x < -0.5 else 0)

# Output summary statistics
print("Merged dataset shape:", df_merged.shape)
print("\nDrought Label Counts:")
print(df_merged["drought_label"].value_counts())

print("\nDrought Label Percentage:")
print(df_merged["drought_label"].value_counts(normalize=True) * 100)

# Save the processed dataset
df_merged.to_csv("prepared_drought_dataset.csv", index=False)
print("\nSaved to 'prepared_drought_dataset.csv'")

# Output summary statistics
print(f"\nâœ… Merged dataset shape: {df_merged.shape}")
print("\nðŸ“Š Drought Label Counts:\n", df_merged["drought_label"].value_counts())
print("\nðŸ“ˆ Drought Label Percentage (%):\n", df_merged["drought_label"].value_counts(normalize=True) * 100)

# Save the processed dataset
output_path = "prepared_drought_dataset.csv"
df_merged.to_csv(output_path, index=False)
print(f"\nðŸ’¾ Saved processed dataset to: {output_path}")

"""
Machine Learning Script for Drought Forecasting
------------------------------------------------
This script loads the prepared drought dataset (created via data_preparation_modelling.py),
selects key features, splits the data into training and testing sets, scales the features,
and prepares them for use in various ML models.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the prepared dataset
df = pd.read_csv("prepared_drought_dataset.csv")

# Define feature columns explicitly
feature_cols = ["mean_precip_mm", "evap_mm"]
X = df[feature_cols]
y = df["drought_label"]

# Train/test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Sanity check
print("âœ… Data split and scaled successfully.")
print(f"Train size: {X_train.shape}, Test size: {X_test.shape}")

"""
Logistic Regression Model for Drought Classification
----------------------------------------------------
This script trains a baseline Logistic Regression model to classify monthly
observations in the Canadian Prairies (Mayâ€“September, 1982â€“2022) as either
'Drought' or 'No Drought' using two key climate features:

- mean_precip_mm
- evap_mm

Key steps:
- Merges CHIRPS, ERA5, and TerraClimate datasets
- Normalizes PDSI and creates binary drought labels
- Splits the dataset (80/20, stratified)
- Applies StandardScaler to input features
- Trains a Logistic Regression model with class_weight='balanced'
- Evaluates using classification report, confusion matrix, accuracy, and ROC-AUC

This serves as a baseline linear model in comparison to more complex algorithms
(Random Forest, XGBoost, SVM) later in the pipeline.
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    roc_auc_score,
    roc_curve
)

# Load the merged dataset (manually prepared from three sources)
df_precip = pd.read_csv("CHIRPS_Monthly_Prairies_1982_2022.csv")
df_evap = pd.read_csv("ERA5_Evapotranspiration_Prairies_1982_2022.csv")
df_pdsi = pd.read_csv("PDSI_TerraClimate_Prairies_1982_2022.csv")

# Merge datasets
df = df_precip.merge(df_evap, on=["year", "month"]).merge(df_pdsi, on=["year", "month"])

# Normalize PDSI and generate drought label
df["pdsi"] = df["pdsi"] / 100
df["drought_label"] = df["pdsi"].apply(lambda x: 1 if x < -0.5 else 0)

# Select features and target
X = df[["mean_precip_mm", "evap_mm"]]
y = df["drought_label"]

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Logistic Regression with class balancing
lr_model = LogisticRegression(class_weight='balanced', random_state=42)
lr_model.fit(X_train_scaled, y_train)

# Predict on test data
y_pred = lr_model.predict(X_test_scaled)
y_proba = lr_model.predict_proba(X_test_scaled)[:, 1]

# Evaluation
print("\n=== Logistic Regression Results ===")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print(f"Accuracy Score: {accuracy_score(y_test, y_pred):.3f}")
print(f"ROC-AUC Score: {roc_auc_score(y_test, y_proba):.3f}")

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure(figsize=(7, 6))
plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc_score(y_test, y_proba):.3f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""
Random Forest Model for Drought Classification
----------------------------------------------
This section trains a Random Forest classifier with hyperparameter tuning
via GridSearchCV using 5-fold cross-validation. The model predicts drought
conditions using two climate variables:

- mean_precip_mm
- evap_mm

Steps:
- Tune hyperparameters using GridSearchCV (scoring = 'f1')
- Use class_weight='balanced' to address class imbalance
- Evaluate performance using classification report, accuracy, and ROC-AUC
- Plot ROC curve for visual interpretation
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    roc_auc_score,
    roc_curve
)
import matplotlib.pyplot as plt

# Define hyperparameter grid
rf_params = {
    'n_estimators': [50, 100],
    'max_depth': [None, 5],
    'min_samples_split': [2, 5]
}

# Initialize Random Forest with class balancing
rf = RandomForestClassifier(class_weight='balanced', random_state=42)

# GridSearchCV with 5-fold cross-validation
rf_grid = GridSearchCV(
    estimator=rf,
    param_grid=rf_params,
    scoring='f1',
    cv=5,
    n_jobs=-1
)

# Fit the model
rf_grid.fit(X_train_scaled, y_train)

# Predictions
best_rf = rf_grid.best_estimator_
y_pred_rf = best_rf.predict(X_test_scaled)
y_proba_rf = best_rf.predict_proba(X_test_scaled)[:, 1]

# Evaluation
print("\n=== Random Forest Results ===")
print("Best Parameters:", rf_grid.best_params_)
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))
print(f"Accuracy Score: {accuracy_score(y_test, y_pred_rf):.3f}")
print(f"ROC-AUC Score: {roc_auc_score(y_test, y_proba_rf):.3f}")

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba_rf)
plt.figure(figsize=(7, 6))
plt.plot(fpr, tpr, label=f'Random Forest (AUC = {roc_auc_score(y_test, y_proba_rf):.3f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""
XGBoost Model for Drought Classification
----------------------------------------
This section trains an XGBoost classifier using 5-fold cross-validation
via GridSearchCV. The model predicts drought vs. no-drought months based on:

- mean_precip_mm
- evap_mm

Steps:
- Tune hyperparameters (n_estimators, max_depth, learning_rate)
- Use F1-score as the scoring metric for GridSearchCV
- Evaluate best model using confusion matrix, classification report, accuracy, and ROC-AUC
- Plot ROC curve for visual performance comparison
"""

from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve
)
import matplotlib.pyplot as plt

# Initialize XGBoost classifier
xgb = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# Define hyperparameter grid
xgb_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1]
}

# GridSearchCV with 5-fold cross-validation
xgb_grid = GridSearchCV(
    estimator=xgb,
    param_grid=xgb_params,
    scoring='f1',
    cv=5,
    n_jobs=-1
)

# Train the model
xgb_grid.fit(X_train_scaled, y_train)

# Predictions
best_xgb = xgb_grid.best_estimator_
y_pred_xgb = best_xgb.predict(X_test_scaled)
y_probs_xgb = best_xgb.predict_proba(X_test_scaled)[:, 1]

# Evaluation
print("\n=== XGBoost Results ===")
print("Best Parameters:", xgb_grid.best_params_)
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))
print(f"Accuracy Score: {accuracy_score(y_test, y_pred_xgb):.3f}")
print(f"ROC AUC Score: {roc_auc_score(y_test, y_probs_xgb):.3f}")

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_probs_xgb)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'XGBoost (AUC = {roc_auc_score(y_test, y_probs_xgb):.3f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - XGBoost')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""
Support Vector Machine (SVM) Model for Drought Classification
-------------------------------------------------------------
This script trains and evaluates an SVM classifier using a radial basis function (RBF) kernel
to predict drought vs. no-drought conditions based on:

- mean_precip_mm
- evap_mm

Steps:
- Hyperparameter tuning via GridSearchCV (C, gamma)
- Use F1-score as the scoring metric
- Enable probability estimates for ROC curve generation
- Evaluate model with classification report, confusion matrix, accuracy, and ROC-AUC
"""

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve
)
import matplotlib.pyplot as plt

# Initialize SVM classifier
svm = SVC(probability=True, random_state=42)

# Hyperparameter grid
svm_params = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.1, 1]
}

# GridSearchCV with 5-fold CV
svm_grid = GridSearchCV(
    estimator=svm,
    param_grid=svm_params,
    scoring='f1',
    cv=5,
    n_jobs=-1
)

# Train the model
svm_grid.fit(X_train_scaled, y_train)

# Best estimator and predictions
best_svm = svm_grid.best_estimator_
y_pred_svm = best_svm.predict(X_test_scaled)
y_probs_svm = best_svm.predict_proba(X_test_scaled)[:, 1]

# Evaluation
print("\n=== SVM Results ===")
print("Best Parameters:", svm_grid.best_params_)
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))
print("\nClassification Report:\n", classification_report(y_test, y_pred_svm))
print(f"Accuracy Score: {accuracy_score(y_test, y_pred_svm):.3f}")
print(f"ROC AUC Score: {roc_auc_score(y_test, y_probs_svm):.3f}")

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_probs_svm)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'SVM (AUC = {roc_auc_score(y_test, y_probs_svm):.3f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - SVM')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()